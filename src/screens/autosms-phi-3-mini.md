# Implementation Document: React Native + Native Android Integration with Phi-3-mini (Microsoft)

## Objective

Enable fully on-device, dynamic Q\&A driven by user-uploaded DOCX/PDF files using the Phi-3-mini LLM. The system must extract relevant passages from uploaded documents, feed them into Phi-3-mini, and generate contextually accurate answers prefixed with “AI:”. All replies must be generated by Phi-3-mini—no static or hard-coded responses.

---

## Architecture Overview

1. **React Native Layer**

   - **UI Components**:
     • File Upload Screen (allows multiple DOCX/PDF selection)
     • Question Input Field (for user queries)
     • Answer Display Area (to show “AI: <dynamic reply>”)
   - **Native Bridge Module**:
     • JS-facing methods to trigger file storage, retrieval, and LLM inference.
     • Watches for questions and returns generated answers.

2. **Native Android Layer**

   - **Document Manager**:
     • Receives URIs of uploaded files from RN.
     • Copies and persists files in app-private storage.
     • Provides access to file paths to Text Extraction module.
   - **Text Extraction Module**:
     • On-demand parses DOCX using Apache POI or similar, PDF using PdfBox-Android.
     • Extracts raw text by paragraph or page; caches segments.
   - **Retrieval Controller**:
     • Receives a user query string.
     • Scores each extracted text segment by relevance to the query (keyword counts or simple embedding).
     • Selects top-N passages to form the context.
   - **Phi-3-mini LLM Engine**:
     • Loads the quantized Phi-3-mini model (e.g., a 4-bit GGUF file) using llama.cpp or MLC LLM.
     • Runs inference on-device, combining the selected context and the query into a prompt.
     • Returns the generated tokens as a UTF-8 string.
   - **SMS Handler (Optional)**:
     • Listens for incoming SMS after a missed call.
     • For each SMS body, triggers the same retrieval + inference pipeline.
     • Sends the LLM’s reply back as an SMS prefixed with “AI:”.

---

## Component Responsibilities and Flow

### 1. Document Manager

- **Responsibilities:**
  • Accept multiple DOCX/PDF URIs from React Native.
  • Persist files in internal storage (e.g., `/data/data/<app>/files/`).
  • Expose stored file paths for downstream extraction.
- **Flow:**

  1. React Native calls `storeUploadedFiles(fileUriList)`.
  2. Document Manager copies each file into private storage, returns a list of absolute file paths.

### 2. Text Extraction Module

- **Responsibilities:**
  • Given a file path, parse and extract text.
  • Tokenize into manageable segments (per paragraph or page).
  • Cache extracted segments in memory or disk for reuse.
- **Flow:**

  1. Receive list of file paths and the current query.
  2. For each file, check cache for existing segments; if none, parse:

     - If DOCX: use Apache POI to read paragraphs.
     - If PDF: use PdfBox-Android to extract text page-by-page.

  3. Store segments in a local cache keyed by filename.
  4. Return all segments to Retrieval Controller.

### 3. Retrieval Controller

- **Responsibilities:**
  • Score each text segment by relevance to the user’s query.
  • Select the top N (e.g., 3–5) segments that best match the query.
  • Provide selected passages to the LLM Engine.
- **Scoring Methods:**
  • **Keyword Matching:** Count overlap of query terms in each segment.
  • **Light Embedding Similarity (Optional):** Compute small embeddings (e.g., TF-IDF or a tiny embedding model) and cosine similarity.
- **Flow:**

  1. Receive `question: String` and `allSegments: List<String>`.
  2. For each segment, compute a relevance score.
  3. Sort segments by descending score; pick top-N.
  4. Return `topSegments: List<String>` to LLM Engine.

### 4. Phi-3-mini LLM Engine

- **Responsibilities:**
  • Load quantized Phi-3-mini model into memory (4-bit GGUF or similar).
  • Accept a concatenated prompt: “Context: <topSegments> Question: <user query> Answer:”.
  • Run generate(inference) on-device and return the generated string.
- **Model Preparation & Integration Options:**

  1. **Quantize Phi-3-mini:**

     - Download original Phi-3-mini checkpoint (3B parameters) from Microsoft/Hugging Face.
     - Use `quantize.py` from llama.cpp to convert to a 4-bit GGUF file (e.g., `phi3mini-4bit.gguf`).

  2. **Runtime Choices:**

     - **llama.cpp:**
       • Compile the llama.cpp Android JNI library with the NDK.
       • Bundle `libllama.so` in `app/src/main/jniLibs/`.
       • Load `phi3mini-4bit.gguf` at startup and call the JNI method `Llama.generate(prompt)`.
     - **MLC LLM:**
       • Add MLC LLM’s AAR to Gradle (e.g., `implementation 'ai.mlc:mlc-android:latest'`).
       • Place `phi3mini-4bit.gguf` in `assets/` or internal storage.
       • Initialize via `MLC.loadModel("phi3mini-4bit.gguf")` and call `MLC.generateAnswer(prompt, settings)` off the UI thread.

- **Flow:**

  1. Native Bridge calls `generateAnswer(question: String)`.
  2. Retrieval Controller returns `topSegments`.
  3. Assemble prompt:

     ```
     "Context: " + join(topSegments, "\n---\n") +
     "\nQuestion: " + question +
     "\nAnswer:"
     ```

  4. Run inference asynchronously:

     - If using llama.cpp: call JNI `Llama.generate(...)`.
     - If using MLC: call `MLC.generateAnswer(...)`.

  5. Wait for generation to complete; capture output string (e.g., “Bleeding heel pivot…”).
  6. Prefix with “AI: ” and return via bridge callback to React Native.

### 5. React Native Bridge

- **Responsibilities:**
  • Expose methods to JavaScript:
  • `storeUploadedFiles(fileUriList: string[]): Promise<string[]>`
  • `askQuestion(question: string): Promise<string>`
  • Listen for callbacks and propagate responses back to JS.
- **Flow:**

  1. JS calls `storeUploadedFiles([...])`; receives stored file paths.
  2. User types question in RN input field; JS calls `askQuestion(question)`.
  3. Bridge triggers native pipeline (Extraction → Retrieval → Inference).
  4. When the LLM response arrives, bridge resolves promise; JS displays answer in UI.

### 6. SMS Auto-Reply (Optional)

- **Responsibilities:**
  • Listen for OS-level SMS_RECEIVED or use a foreground service to detect incoming SMS after a missed call event.
  • Extract the SMS text body and sender number.
  • Pass SMS body to the Q\&A pipeline as the user’s question.
  • Upon receiving “AI: <generated text>”, send SMS back using the native SMS API.
- **Flow:**

  1. BroadcastReceiver detects incoming SMS.
  2. Check if it follows a missed call (you may store state when call ended).
  3. Call `askQuestion(smsBody)`.
  4. When bridge returns the LLM answer, call `SmsManager.sendTextMessage(senderNumber, null, answer, ...)`.

---

## Detailed Implementation Steps

1. **React Native: File Upload UI**

   - Use a file picker library (`react-native-document-picker`) to allow selection of multiple DOCX/PDF files.
   - Upon selection, call `storeUploadedFiles([...uris])` on the native bridge.
   - Display a summary of stored files and their upload status.

2. **Native Android: Document Manager**

   - In your Android module, implement a method `storeUploadedFiles(List<String> uris)`:
     • For each URI, copy the file to `getFilesDir() + "/uploaded/" + <uniqueName>.pdf/docx`.
     • Return a list of absolute paths to React Native.

3. **Native Android: Text Extraction Module**

   - Add dependencies:
     • **PDF Parsing:** `com.tom-roush:pdfbox-android:2.X.X`
     • **DOCX Parsing:** `org.apache.poi:poi-ooxml:5.X.X`
   - Implement a method `extractSegments(List<String> filePaths): List<String>`:
     • For each PDF: read each page, `stripText = PDFTextStripper.getText(...)` → split by paragraphs.
     • For each DOCX: open with `XWPFDocument`, read each `XWPFParagraph`, collect text.
     • Cache \[] of segments keyed by file path in a `Map<String, List<String>>`.

4. **Native Android: Retrieval Controller**

   - Implement `getTopSegments(String question, List<String> allSegments): List<String>`:
     • Convert question to lowercase; tokenize by whitespace.
     • For each segment: convert to lowercase, count matching tokens.
     • Score = count of overlapping words; sort descending; pick top 3–5.

5. **Native Android: Phi-3-mini Model Preparation**

   1. **Download & Quantize Weights:**

      - Download `phi3-mini.pth` or equivalent from Hugging Face.
      - Run on your workstation:

        ```bash
        python quantize.py phi3-mini.pth phi3-mini-4bit.gguf 4
        ```

      - Confirm the resulting file size (≈3–4 GB).

   2. **Bundle in APK:**

      - Place `phi3-mini-4bit.gguf` into `app/src/main/assets/`.
      - If the APK exceeds size limits, host the file remotely and download on first launch.

6. **Native Android: Integrate llama.cpp or MLC LLM Runtime**

   - **Option A: llama.cpp**

     1. Add llama.cpp as a Git submodule or copy JNI folder.
     2. Update `android/app/build.gradle`:

        ```groovy
        externalNativeBuild {
            cmake {
                path "src/main/cpp/CMakeLists.txt"
            }
        }
        ndkVersion "<your NDK version>"
        ```

     3. In `CMakeLists.txt`, configure to compile llama.cpp into `libllama.so`.
     4. Load the library at runtime:

        ```java
        static { System.loadLibrary("llama"); }
        ```

     5. Expose a JNI wrapper method `String generateWithPhi3(String prompt)` that calls into llama.cpp’s API to `LlamaCreateFromGGUF(…)` and `LlamaGenerate(...)`.

   - **Option B: MLC LLM**

     1. Add to Gradle:

        ```groovy
        implementation 'ai.mlc:mlc-android:<latest-version>'
        ```

     2. In native code, initialize MLC:

        ```java
        MLC.loadModel(getAssets().open("phi3-mini-4bit.gguf"));
        ```

     3. Expose a method `String generateWithPhi3(String prompt)` that calls:

        ```java
        String answer = MLC.generateAnswer(prompt, new GenerateOptions(...));
        ```

7. **Native Android: Inference Pipeline**

   - Implement `askQuestion(String question): String`:

     1. Call `extractSegments(storedFilePaths)`.
     2. Call `getTopSegments(question, allSegments)`.
     3. Build `prompt = "Context: " + join(topSegments, "\n---\n") + "\nQuestion: " + question + "\nAnswer:"`.
     4. Call `generateWithPhi3(prompt)` on a background thread.
     5. Wait for output; if empty or null, set fallback:

        ```
        answer = "AI: I’m sorry, I couldn’t find an answer in your documents. Please try another question.";
        ```

     6. Otherwise, prefix: `answer = "AI: " + generatedText;`.
     7. Return `answer` via the JS promise callback.

8. **React Native: Bridge Implementation**

   - Expose two native methods through React Native’s `@ReactMethod`:
     • `Promise storeUploadedFiles(ReadableArray uriArray)`
     • `Promise askQuestion(String question)`
   - In JS, call `NativeModules.Phi3Bridge.storeUploadedFiles(...)` after file selection.
   - Bind `askQuestion(question)` to the Submit button in the UI; on promise resolve, update state to display answer.

9. **SMS Auto-Reply Integration (Optional)**

   - Register a `BroadcastReceiver` for incoming SMS.
   - Ensure you have `RECEIVE_SMS` and `SEND_SMS` permissions.
   - On SMS arrival (and only if there was a recent missed call), extract the SMS body.
   - Call `askQuestion(smsBody)` to get the dynamic LLM response.
   - Use `SmsManager.sendTextMessage(senderNumber, null, aiAnswer, null, null);` to reply.

---

## Non-Functional Considerations

1. **Performance & Memory**

   - A 4-bit quantized 3 B model consumes \~2–3 GB of RAM. Test on target devices to avoid OOM.
   - Run inference off the main thread (use `AsyncTask` or Kotlin Coroutines).
   - Consider unloading the model when idle to free memory.

2. **APK Size & Distribution**

   - A 4-bit GGUF file (\~3–4 GB) may be too large to bundle. Instead, download it on first launch and verify with a checksum.
   - Use `android:extractNativeLibs="true"` to include large native libraries without pushing up install time.

3. **Privacy & Security**

   - All processing (extraction, retrieval, inference) occurs locally—no external API calls.
   - Store user documents securely in internal storage—not world-readable.

4. **Extensibility**

   - Discovery: You can later replace the retrieval logic with an embedding-based vector index.
   - Model Updates: To update to a new Phi-3 variant, simply bundle a new quantized file.

5. **Error Handling & Fallbacks**

   - If text extraction fails, return:
     "AI: I couldn’t read the uploaded documents. Please upload valid PDF or DOCX files."
   - If retrieval yields zero matches, return:
     "AI: I couldn’t find any relevant information. Please try a different question."
   - If inference crashes or device is low on memory, return:
     "AI: Sorry, I’m not available right now."
